{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46645a4b1b4847b5a16e899a6c14b2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4fc0c0749b7421b9446500f99ea419e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8b737656aa064decb78764a8e857621d",
              "IPY_MODEL_02f136b5b70949d98b62d2a93e7c7b0a"
            ]
          }
        },
        "a4fc0c0749b7421b9446500f99ea419e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b737656aa064decb78764a8e857621d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01d2dcc571564589937e8585a885113b",
            "_dom_classes": [],
            "description": "Processing...:  52%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 639,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 332,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfc940038a234df2941dfa928de6ce37"
          }
        },
        "02f136b5b70949d98b62d2a93e7c7b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a46d8b1446b46e29ad48de691758486",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 332/639 [01:09&lt;00:55,  5.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2934bc26882c407680c1af7d12f72698"
          }
        },
        "01d2dcc571564589937e8585a885113b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfc940038a234df2941dfa928de6ce37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a46d8b1446b46e29ad48de691758486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2934bc26882c407680c1af7d12f72698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U50neBhXXc2"
      },
      "source": [
        "# **Initial Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S96m0aWsV2Os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9b1856-a540-4107-a749-f5dbc5836f98"
      },
      "source": [
        "!gdown --id 1--R75ERdRPjwh2XHWiRHno1FIuuA3iVg --output cv2.cpython-37m-x86_64-linux-gnu.so #enable GPU support\n",
        "\n",
        "!gdown --id 19yjsdPXQ2DQURD1xqzYHrGTdhMbgGywL --output pedestrian.mp4\n",
        "!gdown --id 1jgMqCnnZ5PYQxFHvJRp8W8URcQhohbJZ --output coco.names\n",
        "!gdown --id 1UuzgjSOwLJHE6FrGjluwh9ORyknlJfBw --output yolov3.cfg\n",
        "!gdown --id 1qlBKyUChyvkMc3YcSnc_4JpcxeS-93lY --output yolov3.weights\n",
        "!gdown --id 1AW0Jiw6m7iDHS6XkTVDhnSMzWKqGKH5S --output static_frame_from_video.jpg  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--R75ERdRPjwh2XHWiRHno1FIuuA3iVg\n",
            "To: /content/cv2.cpython-37m-x86_64-linux-gnu.so\n",
            "1.01GB [00:04, 234MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19yjsdPXQ2DQURD1xqzYHrGTdhMbgGywL\n",
            "To: /content/pedestrian.mp4\n",
            "7.11MB [00:00, 61.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jgMqCnnZ5PYQxFHvJRp8W8URcQhohbJZ\n",
            "To: /content/coco.names\n",
            "100% 624/624 [00:00<00:00, 1.15MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UuzgjSOwLJHE6FrGjluwh9ORyknlJfBw\n",
            "To: /content/yolov3.cfg\n",
            "100% 8.36k/8.36k [00:00<00:00, 7.15MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qlBKyUChyvkMc3YcSnc_4JpcxeS-93lY\n",
            "To: /content/yolov3.weights\n",
            "248MB [00:02, 124MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AW0Jiw6m7iDHS6XkTVDhnSMzWKqGKH5S\n",
            "To: /content/static_frame_from_video.jpg\n",
            "100% 314k/314k [00:00<00:00, 40.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LXkoyQy5BdW"
      },
      "source": [
        "# **Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQTySQB5a8L"
      },
      "source": [
        "import cv2 as cv #OpenCV Library\n",
        "import numpy as np  #for handling arrays\n",
        "from scipy.spatial import distance  #for cdist \n",
        "#from tqdm.std import tqdm  #for system progressbar\n",
        "from tqdm.notebook import tqdm #for google colab progressbar\n",
        "from time import time # for testing purposes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdTKV6i5lKv"
      },
      "source": [
        "# **Total frames counter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoawRcxn6Q_I"
      },
      "source": [
        "def total_frames(file_name):\n",
        "    cap = cv.VideoCapture(file_name)\n",
        "    res = 0\n",
        "\n",
        "    while True:\n",
        "        ret,img = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        res = res+1\n",
        "                                            \n",
        "    return res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMhrkVgG6ipL"
      },
      "source": [
        "# **YOLO object detection function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_46JuEP6tBX"
      },
      "source": [
        "def object_detection_YOLO(img,threshold,nms_threshold):\n",
        "    # determine the output layers\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    # construct a blob from the image\n",
        "    # blob is just a preprocessed image\n",
        "    blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)      #blob = boxes\n",
        "    # blobs goes as the input to YOLO\n",
        "    # inputting blob to the Neural Network\n",
        "    net.setInput(blob)\n",
        "    # t0 = time.time()\n",
        "    outputs = net.forward(ln)   # finds output\n",
        "    # t = time.time()\n",
        "    # print('time=', t-t0)\n",
        "\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    centroids = []\n",
        "    results = []\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    for output in outputs:  # Outputs have all the detection and their probability for every class\n",
        "        for detection in output:    # detection is the the list of all probabilities with box dimension in start\n",
        "            scores = detection[5:]  # everything in array after 5th element\n",
        "            classID = np.argmax(scores)     # picks the maximum probability\n",
        "            confidence = scores[classID] \n",
        "\n",
        "            if (confidence > threshold) and (classID == 0):\n",
        "                #first 4 elemensts are box characteristics normalized to range(0,1)\n",
        "                #first two element are middle co-ordinate\n",
        "                # next two are width and height of blob           \n",
        "                box = detection[:4] * np.array([w, h, w, h])    \n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")   # typecasting to int, as array indexes are int\n",
        "                x = int(centerX - (width / 2))      # finding upper corner\n",
        "                y = int(centerY - (height / 2))\n",
        "                box = [x, y, int(width), int(height)]   # changing origin to top left and typecasted to int\n",
        "                boxes.append(box)                       # added the box to boxes\n",
        "                confidences.append(float(confidence))   # added confidence to confidences\n",
        "                centroids.append((centerX,centerY))\n",
        "\n",
        "    indices = cv.dnn.NMSBoxes(boxes, confidences,score_threshold=threshold,nms_threshold=nms_threshold)\n",
        "    # score_threshold -> threshold for confidence\n",
        "    # nms_threshold -> threshold for how close to blobs are, if two blobs are too close, one of them is discarded\n",
        "    # closeness is determined by IoU (intersection over Union)\n",
        "    # discarding is based on confidence, higher confidence is retained\n",
        "\n",
        "    boxes_final=[]; confidences_final=[]; centroids_final=[]\n",
        "    if len(indices):\n",
        "        for i in indices.flatten():\n",
        "            # extract the bounding box coordinates\n",
        "            x, y = (boxes[i][0], boxes[i][1])\n",
        "            w, h = (boxes[i][2], boxes[i][3])\n",
        "            boxes_final.append((x,y,w,h))\n",
        "            confidences_final.append(confidences[i])\n",
        "            centroids_final.append(centroids[i])\n",
        "\n",
        "    return boxes_final,confidences_final,centroids_final"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNX04kdZ7qw4"
      },
      "source": [
        "# **Bird's Eye Perspective**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DOTC5zA7yaz"
      },
      "source": [
        "def birds_eye_view(corner_points,width,height,image):\n",
        "    \"\"\"\n",
        "    Compute the transformation matrix\n",
        "    corner_points : 4 corner points selected from the image\n",
        "    height, width : size of the image\n",
        "    return : transformation matrix and the transformed image\n",
        "    \"\"\"\n",
        "    # Create an array out of the 4 corner points\n",
        "    corner_points = np.float32(corner_points)\n",
        "    # Create an array with the parameters (the dimensions) required to build the matrix\n",
        "    img_params = np.float32([[0,0],[width,0],[0,height],[width,height]])\n",
        "    # Compute and return the transformation matrix\n",
        "    matrix = cv.getPerspectiveTransform(corner_points,img_params)\n",
        "    img_transformed = cv.warpPerspective(image,matrix,(width,height))\n",
        "    \n",
        "    return matrix,img_transformed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbr5SnMW8HVm"
      },
      "source": [
        "# **Align The Centroids to the Bird's Eye Perspective**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svTWaBRd8OVV"
      },
      "source": [
        "def birds_eye_point(matrix,centroids):\n",
        "    \"\"\" Apply the perspective transformation to every ground point which have been detected on the main frame.\n",
        "    @ matrix : the 3x3 matrix\n",
        "    @ centroids : list that contains the points to transform\n",
        "    return : list containing all the new points\n",
        "    \"\"\"\n",
        "    # Compute the new coordinates of our points\n",
        "    points = np.float32(centroids).reshape(-1, 1, 2)\n",
        "    transformed_points = cv.perspectiveTransform(points, matrix)\n",
        "    # Loop over the points and add them to the list that will be returned\n",
        "    transformed_points_list = list()\n",
        "\n",
        "    for i in range(0,transformed_points.shape[0]):\n",
        "        transformed_points_list.append([transformed_points[i][0][0],transformed_points[i][0][1]])\n",
        "        \n",
        "    return transformed_points_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEjFFfpp8eUz"
      },
      "source": [
        "# **File setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F21TtZnv8iYh"
      },
      "source": [
        "file_name = \"pedestrian.mp4\"\n",
        "tot_frame = total_frames(file_name)\n",
        "cap = cv.VideoCapture(file_name)\n",
        "\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv.VideoWriter('output.mp4',fourcc, 25.0,(1282,400))\n",
        "\n",
        "# Load names of classes and get random colors (not needed because we only care about people)\n",
        "# with open(\"coco.names\") as f:\n",
        "#     classes = f.read().strip().split(\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FodqFTJE83DH"
      },
      "source": [
        "# **Network setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q15JjLPh86Nd"
      },
      "source": [
        "# Give the configuration and weight files for the model and load the network.\n",
        "net = cv.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')  # Reads Network from .cfg and .weights\n",
        "net.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)   # this specifies what type of hardware to use (GPU or CPU)\n",
        "net.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA)     # sets preferable hardware\n",
        "\n",
        "threshold = 0.75\n",
        "nms_threshold = 0.6\n",
        "distance_px=120   # arbitrary value for now but looked best"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Oy4SzNAj2a"
      },
      "source": [
        "# **Bird Eye View Transform Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHOh_MkeAjjh"
      },
      "source": [
        "########################### Corner points #############################\n",
        "# top-left: (775,10)\n",
        "# top-right: (1270,60)\n",
        "# bottom-left: (0,350)\n",
        "# bottom-right: (1100,700)\n",
        "corners=[(0,0),(1280,0),(0,720),(1280,720)]  # these are the best looking coordinate I got via hit and trial\n",
        "\n",
        "tl,tr,bl,br=corners   #top-left, top-right, bottom-left, bottom-right\n",
        "\n",
        "width1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "width2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "width_final = max(int(width1), int(width2))\n",
        "\n",
        "height1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "height2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "height_final = max(int(height1), int(height2))\n",
        "\n",
        "static_frame=cv.imread(\"static_frame_from_video.jpg\")\n",
        "static_frame=cv.resize(static_frame,(1280,720))  # these are the dimensions we are using for the video\n",
        "\n",
        "persp_matrix, transformed_img = birds_eye_view(corners, width_final, height_final, static_frame)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2_XU0NL2uAL"
      },
      "source": [
        "# **Making Photos For Heading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYT3CBa21Ca"
      },
      "source": [
        "camera_view_heading = np.zeros((40,640,3),np.uint8)\n",
        "camera_view_heading_text = \"Camera View\"\n",
        "white = (255,255,255)\n",
        "camera_view_heading = cv.putText(camera_view_heading, camera_view_heading_text, (220, camera_view_heading.shape[0]-13), cv.FONT_HERSHEY_SIMPLEX,0.85, white,2)\n",
        "\n",
        "bird_eye_heading = np.zeros((40,640,3),np.uint8)\n",
        "bird_eye_heading_text = \"Bird-Eye View\"\n",
        "bird_eye_heading = cv.putText(bird_eye_heading, bird_eye_heading_text, (220, bird_eye_heading.shape[0]-13), cv.FONT_HERSHEY_SIMPLEX,0.85, white,2)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXmM51nO9Rdq"
      },
      "source": [
        "# **Process the input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mUS1Le59eR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "46645a4b1b4847b5a16e899a6c14b2eb",
            "a4fc0c0749b7421b9446500f99ea419e",
            "8b737656aa064decb78764a8e857621d",
            "02f136b5b70949d98b62d2a93e7c7b0a",
            "01d2dcc571564589937e8585a885113b",
            "bfc940038a234df2941dfa928de6ce37",
            "5a46d8b1446b46e29ad48de691758486",
            "2934bc26882c407680c1af7d12f72698"
          ]
        },
        "outputId": "dddee67c-d6f9-43a5-a302-2ca85fe4c1de"
      },
      "source": [
        "initial_time=time()\n",
        "for i in tqdm (range (tot_frame), desc=\"Processing...\"): \n",
        "    ret,img = cap.read()\n",
        "    if not ret: break\n",
        "\n",
        "    birds_display=cv.warpPerspective(img,persp_matrix,(width_final,height_final))\n",
        "        \n",
        "    boxes,confidences,centroids=object_detection_YOLO(img, threshold, nms_threshold)\n",
        "    # box -> x,y,w,h\n",
        "    # confidence -> confidence of the detected object\n",
        "    # centroid -> center of the bbox, 2 values list \n",
        "\n",
        "    detections=len(boxes)\n",
        "\n",
        "    # violate=set()  # instead of a set, we can use a dictionary to speed stuff up.\n",
        "    violate={}\n",
        "\n",
        "    if detections>1:  # to check if there are at least two people in the frame, otherwise no need to run the algorithm\n",
        "\n",
        "        transformed_centroids=birds_eye_point(persp_matrix,centroids)\n",
        "        transformed_centroids=np.array([(int(x),int(y)) for x,y in transformed_centroids])\n",
        "\n",
        "        # calculates the distance between all the pairs of points\n",
        "        D=distance.cdist(transformed_centroids,transformed_centroids,metric=\"euclidean\")\n",
        "\n",
        "        for i in range(D.shape[0]):\n",
        "            for j in range(i+1, D.shape[1]):\n",
        "            # check to see if the distance between any two\n",
        "            # centroid pairs is less than the configured number\n",
        "            # of pixels\n",
        "                if D[i, j]<distance_px:\n",
        "                    # update our violation set with the indexes of\n",
        "                    # the centroid pairs\n",
        "                    # violate.add(i)\n",
        "                    # violate.add(j)\n",
        "                    violate[i]=1\n",
        "                    violate[j]=1\n",
        "                    \n",
        "        for i in range(detections):\n",
        "            x, y = boxes[i][0], boxes[i][1]\n",
        "            w, h = boxes[i][2], boxes[i][3]\n",
        "            startX, startY, endX, endY = x,y,x+w,y+h\n",
        "            color = (0, 255, 0)  # green\n",
        "            # if the index pair exists within the violation set, then\n",
        "            # update the color\n",
        "            # if i in violate: color=(0, 0, 255)  # red\n",
        "            if violate.get(i) is not None: color=(0, 0, 255)\n",
        "            # draw (1) a bounding box around the person and (2) the\n",
        "            # centroid coordinates of the person,\n",
        "            img = cv.rectangle(img, (startX, startY), (endX, endY), color, 2)\n",
        "            img = cv.circle(img,(centroids[i][0],centroids[i][1]),1,color,10)\n",
        "            birds_display = cv.circle(birds_display,(transformed_centroids[i][0],transformed_centroids[i][1]),1,color,10)\n",
        "\n",
        "        # display the rectangle where the bird's magic is happening\n",
        "        blue = (255,0,0)\n",
        "        img = cv.line(img,tl,tr,blue,2)\n",
        "        img = cv.line(img,tl,bl,blue,2)\n",
        "        img = cv.line(img,bl,br,blue,2)\n",
        "        img = cv.line(img,tr,br,blue,2)\n",
        "\n",
        "    # draw the total number of social distancing violations on the output frame\n",
        "    text = \"Social Distancing Violations: {}\".format(len(violate))\n",
        "    img = cv.putText(img, text, (10, img.shape[0]-25), cv.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
        "\n",
        "    output = np.zeros((400,1282,3),img.dtype)\n",
        "\n",
        "    img_half = cv.resize(img,(640,360)) \n",
        "    output[0:40,0:640,0:3] = camera_view_heading\n",
        "    output[40:400,0:640,0:3] = img_half\n",
        "\n",
        "    birds_display_half = cv.resize(birds_display,(640,360))\n",
        "    output[0:40,642:1282,0:3] = bird_eye_heading\n",
        "    output[40:400,642:1282,0:3] = birds_display_half\n",
        "\n",
        "    out.write(output)\n",
        "  \n",
        "cap.release()\n",
        "out.release()\n",
        "print(\"Processing Completed, Download 'output.mp4' to View Results\")\n",
        "print(f\"Time taken to process the input: {time()-initial_time} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46645a4b1b4847b5a16e899a6c14b2eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Processing...', max=639.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}